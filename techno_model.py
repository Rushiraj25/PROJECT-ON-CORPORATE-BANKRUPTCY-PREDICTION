# -*- coding: utf-8 -*-
"""techno_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12cC-LxHUPTQOGM2hkyp1Ww0TjYA1okdo
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/drive')

bank_train = pd.read_csv('/content/drive/MyDrive/data/training_data.csv')
bank_test = pd.read_csv('/content/drive/MyDrive/data/test_data.csv')

bank = pd.concat([bank_train,bank_test],axis=0)

X = bank.drop(columns = ['class'])
y = bank[['class']]

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2)

print(X_train.shape)
print(X_test.shape)

from sklearn.preprocessing import MinMaxScaler
mns = MinMaxScaler()

mns.fit(X_train,y_train)
X_train_scaled = pd.DataFrame(mns.transform(X_train),columns = X_train.columns)
X_test_scaled = pd.DataFrame(mns.transform(X_test),columns = X_test.columns)

from sklearn.feature_selection import SelectKBest,chi2
features = SelectKBest(score_func=chi2)

topbest = features.fit(X_train_scaled, y_train)

bank_scores =pd.DataFrame(topbest.scores_)
bank_columns = pd.DataFrame(X_train_scaled.columns)

fea_scores = pd.concat([bank_scores,bank_columns],axis = 1)

fea_scores.columns = ['max_score','column']

fea_scores

fea_scores.nlargest(n = 15,columns='max_score')['column']

X_train = X_train[fea_scores.nlargest(n = 15,columns='max_score')['column']]
X_test = X_test[fea_scores.nlargest(n = 15,columns='max_score')['column']]

#from sklearn.svm import SVC
# Building a Support Vector Machine on train data
#svc_model = SVC(C= .1, kernel='linear', gamma= 1)
#svc_model.fit(X_train, y_train)

#model_linear = SVC(kernel = "linear")
#model_linear.fit(X_train, y_train)
#pred_test_linear = model_linear.predict(X_test)

#np.mean(pred_test_linear == y_test)

#prediction = svc_model .predict(X_test)
# check the accuracy on the training set
#print(svc_model.score(X_train, y_train))
#print(svc_model.score(X_test, y_test))

import xgboost as xgb

xgb_clf = xgb.XGBClassifier(max_depths = 5, n_estimators = 10000, learning_rate = 0.3, n_jobs = -1)
# n_jobs – Number of parallel threads used to run xgboost.
# learning_rate (float) – Boosting learning rate (xgb’s “eta”)


xgb_clf.fit(X_train, y_train)

from sklearn.metrics import accuracy_score, confusion_matrix

# Evaluation on Testing Data
confusion_matrix(y_test, xgb_clf.predict(X_test))
#accuracy_score(y_test, xgb_clf.predict(X_test))

#xgb.plot_importance(xgb_clf)

accuracy_score(y_test, xgb_clf.predict(X_test))

#! pip install lightgbm

import lightgbm as lgb
  
# Similarly LGBMRegressor can also be imported for a regression model.
from lightgbm import LGBMClassifier

# Creating an object for model and fitting it on training data set 
model = lgb.LGBMClassifier()
model.fit(X_train, y_train)

# Predicting the Target variable
y_pred = model.predict(X_test)
print(y_pred)

accuracy = model.score(X_test, y_test)
print(accuracy)

from sklearn.metrics import accuracy_score
accuracy=accuracy_score(y_pred, y_test)
print('LightGBM Model accuracy score: {0:0.4f}'.format(accuracy_score(y_test, y_pred)))

y_pred_train = model.predict(X_train)
print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))

from sklearn.metrics import classification_report
print(classification_report(y_test,y_pred))

import pickle

Pkl_Filename = "Pickle_LGBM_Model.pkl"  

with open(Pkl_Filename, 'wb') as file:  
    pickle.dump(model, file)

with open(Pkl_Filename, 'rb') as file:  
    Pickled_Lgbm_Model = pickle.load(file)

Pickled_Lgbm_Model

import os
cwd = os.getcwd()

cwd

